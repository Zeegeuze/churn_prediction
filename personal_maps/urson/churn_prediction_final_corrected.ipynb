{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3028932",
   "metadata": {},
   "source": [
    "# **Churn Prediction: Final Refined Version**\n",
    "This notebook implements data preprocessing, thorough EDA, and model evaluation for churn prediction using Random Forest and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d782f",
   "metadata": {},
   "source": [
    "## **Step 1: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5dc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c76565",
   "metadata": {},
   "source": [
    "## **Step 2: Load and Inspect Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('BankChurners.csv')\n",
    "\n",
    "# Drop CLIENTNUM and Naive Bayes classifier columns\n",
    "data.drop(columns=['CLIENTNUM', \n",
    "                   'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n",
    "                   'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n",
    "                   'Avg_Open_To_Buy'],\n",
    "          inplace=True)\n",
    "\n",
    "# Confirm updated features\n",
    "print(\"Updated Feature Set:\", data.columns)\n",
    "\n",
    "# Overview\n",
    "print(\"Data Overview:\")\n",
    "display(data.head())\n",
    "print(\"Shape of data:\", data.shape)\n",
    "print(\"Null Values:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e20a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Data Preprocessing - Clean Data and Drop Redundant Features\n",
    "# Drop redundant feature 'Credit_Limit'\n",
    "data = data.drop(columns=['Credit_Limit'], errors='ignore')  # Avoid KeyError if already dropped\n",
    "print(\"Dropped 'Credit_Limit'. Remaining columns:\")\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413fc3f5",
   "metadata": {},
   "source": [
    "## **Step 3: Encode Categorical Columns and Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode target variable and binary features\n",
    "data['Attrition_Flag'] = data['Attrition_Flag'].replace({'Attrited Customer': 1, 'Existing Customer': 0})\n",
    "data['Gender'] = data['Gender'].replace({'F': 1, 'M': 0})\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_cols = ['Education_Level', 'Income_Category', 'Marital_Status', 'Card_Category']\n",
    "for col in categorical_cols:\n",
    "    if 'Unknown' in data[col].unique():\n",
    "        data = pd.concat([data, pd.get_dummies(data[col], prefix=col).drop(columns='{}_Unknown'.format(col))], axis=1)\n",
    "    else:\n",
    "        data = pd.concat([data, pd.get_dummies(data[col], prefix=col)], axis=1)\n",
    "\n",
    "# Drop original categorical columns\n",
    "data.drop(columns=categorical_cols, inplace=True)\n",
    "\n",
    "print(\"Cleaned Data:\")\n",
    "display(data.head())\n",
    "print(\"Updated Shape:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03928511",
   "metadata": {},
   "source": [
    "## **Step 4: Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize class imbalance\n",
    "sns.countplot(x='Attrition_Flag', data=data)\n",
    "plt.title(\"Churn Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplots for numerical features to identify patterns\n",
    "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_features, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.boxplot(y=data[col], x=data['Attrition_Flag'])\n",
    "    plt.title(f\"{col} by Churn\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot for Total_Trans_Ct vs Total_Trans_Amt\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='Total_Trans_Ct', y='Total_Trans_Amt', data=data)\n",
    "plt.title('Relationship Between Transaction Count and Amount')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27af97",
   "metadata": {},
   "source": [
    "## **Step 5: Train-Test Split and Class Balancing with SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c26a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data\n",
    "X = data.drop(columns=['Attrition_Flag'])\n",
    "y = data['Attrition_Flag']\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# SMOTE for imbalance handling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Class Distribution After SMOTE:\")\n",
    "print(y_train_res.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f6243",
   "metadata": {},
   "source": [
    "## **Step 6: Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf):.2f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_rf):.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fade5e",
   "metadata": {},
   "source": [
    "## **Step 7: XGBoost Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c593cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(scale_pos_weight=6, n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.2f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_xgb):.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_xgb):.2f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_xgb):.2f}\")\n",
    "print(\"Classification Report:\", classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca05d23",
   "metadata": {},
   "source": [
    "## **Step 8: Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eea1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = xgb_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fda00",
   "metadata": {},
   "source": [
    "## **Step 9: Feature Redundancy Test**\n",
    "In this section, we evaluate whether `Total_Trans_Amt` can be safely dropped without significantly impacting the model performance. This is done by training and comparing the XGBoost model with and without the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd47f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Redefine features and target variable to avoid earlier modifications\n",
    "X = data.drop(columns=['Attrition_Flag'])  # All features\n",
    "y = data['Attrition_Flag']                # Target variable\n",
    "\n",
    "# Section: Evaluating Redundant Features (Total_Trans_Ct vs Total_Trans_Amt)\n",
    "\n",
    "# 1. Train model with both features\n",
    "X_full = X.copy()  # Original features\n",
    "y_full = y.copy()\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size=0.3, random_state=42, stratify=y_full)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res_full, y_train_res_full = smote.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "# Train XGBoost with both features\n",
    "xgb_full = XGBClassifier(scale_pos_weight=6, n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "xgb_full.fit(X_train_res_full, y_train_res_full)\n",
    "y_pred_full = xgb_full.predict(X_test_full)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model Performance with Both Features:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_full, y_pred_full):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test_full, y_pred_full):.2f}\")\n",
    "\n",
    "# 2. Train model without Total_Trans_Amt\n",
    "X_reduced = X.drop(columns=['Total_Trans_Amt'])  # Drop redundant feature\n",
    "\n",
    "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_reduced, y_full, test_size=0.3, random_state=42, stratify=y_full)\n",
    "\n",
    "# Handle class imbalance\n",
    "X_train_res_red, y_train_res_red = smote.fit_resample(X_train_red, y_train_red)\n",
    "\n",
    "# Train XGBoost without Total_Trans_Amt\n",
    "xgb_red = XGBClassifier(scale_pos_weight=6, n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "xgb_red.fit(X_train_res_red, y_train_res_red)\n",
    "y_pred_red = xgb_red.predict(X_test_red)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nModel Performance Without Total_Trans_Amt:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_red, y_pred_red):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test_red, y_pred_red):.2f}\")\n",
    "\n",
    "# 3. Compare results\n",
    "print(\"\\nComparison of Model Performance:\")\n",
    "print(f\"Accuracy Difference: {accuracy_score(y_test_full, y_pred_full) - accuracy_score(y_test_red, y_pred_red):.4f}\")\n",
    "print(f\"F1-Score Difference: {f1_score(y_test_full, y_pred_full) - f1_score(y_test_red, y_pred_red):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba60d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Relative Change Features\n",
    "data['Rel_Amt_Change'] = data['Total_Amt_Chng_Q4_Q1'] / (data['Total_Trans_Amt'] + 1)  # Avoid division by zero\n",
    "data['Rel_Ct_Change'] = data['Total_Ct_Chng_Q4_Q1'] / (data['Total_Trans_Ct'] + 1)\n",
    "\n",
    "# Inspect the new features\n",
    "print(data[['Rel_Amt_Change', 'Rel_Ct_Change']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_change_customers = data[\n",
    "    (data['Rel_Amt_Change'] > data['Rel_Amt_Change'].quantile(0.95)) | \n",
    "    (data['Rel_Ct_Change'] > data['Rel_Ct_Change'].quantile(0.95))\n",
    "]\n",
    "print(\"High Change Customers:\\n\", high_change_customers['Attrition_Flag'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
